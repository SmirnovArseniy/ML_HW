{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1: линейная регрессия и векторное дифференцирование (10 баллов).\n",
    "\n",
    "* Максимальное количество баллов за задания в ноутбуке - 11, но больше 10 оценка не ставится, поэтому для получения максимальной оценки можно сделать не все задания.\n",
    "\n",
    "* Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим многомерную регрессию из sklearn для стандартного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.892056207699224e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.870036668388616e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.74221905e-09, -4.17724606e-08, -1.79051871e-08, -8.79903529e-09,\n",
       "       -3.17528632e-10, -6.68193091e-10, -7.28235946e-09, -1.08742286e-08,\n",
       "        2.16586724e-10,  9.18615841e-08, -1.78301001e-09,  3.91326818e+01,\n",
       "       -5.07349337e-08,  1.05284678e-08, -1.56047650e-08, -7.09338778e-08,\n",
       "       -6.31623867e-09, -1.75350413e-09,  5.36243315e-09,  2.21020084e-11,\n",
       "        3.33239287e-08, -2.97911061e-08,  3.56629148e-08, -6.11159535e-08,\n",
       "       -5.31343092e-10,  5.46858996e+01,  5.39089391e-08,  1.91044452e-08,\n",
       "        6.83750874e-08,  9.00626857e+01,  2.82383751e-08, -8.88254933e-09,\n",
       "        8.20042253e-10,  2.91540252e-08, -6.05131384e-08, -1.94689944e-08,\n",
       "       -2.45091921e-08,  6.96162624e-08,  1.33923693e-08,  3.41558054e+01,\n",
       "        5.36032341e-08,  6.45387341e-08,  1.73950002e-08,  8.60891719e-08,\n",
       "        5.56214324e-09, -2.21727122e-08,  7.85650818e-08, -2.38166763e-08,\n",
       "        4.30995304e+01, -9.24404100e-09, -1.26308883e-08,  9.07501988e-08,\n",
       "       -4.45844925e-09,  6.87823120e+01, -1.94263469e-08, -5.29342968e-08,\n",
       "        4.46020887e+01,  4.12157575e-08,  3.43056656e-08,  1.25256506e-08,\n",
       "        4.96515263e-08, -3.13451302e-08, -2.42948835e-08, -1.07577882e-09,\n",
       "        3.54226307e-09, -1.57375336e-08,  1.46201020e-08, -9.56919278e-08,\n",
       "       -1.45399121e-08,  6.74570384e-08, -9.86264520e-10,  2.81611175e-08,\n",
       "       -9.42943367e-09, -2.08463169e-08,  1.08488290e-08, -3.96069589e-08,\n",
       "       -1.45898961e-08,  6.75626301e-09,  6.02171014e-08,  2.31327919e-08,\n",
       "       -6.59352153e-09,  4.29679126e-08,  1.55337994e-08,  8.78111182e+01,\n",
       "       -6.58465700e-08, -2.77437072e-08, -1.90057567e-08, -1.97177035e-09,\n",
       "        2.98119119e-08, -3.98439693e-08,  4.85162720e-08,  2.46200075e-08,\n",
       "       -7.63627204e-08, -1.25304895e-07, -2.21475387e-09,  4.22604505e-08,\n",
       "        3.31440466e+01,  3.49796389e-08,  9.22765726e+01,  3.49057805e-09])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 1 (0.5 балла).*** Объясните, чем вызвано различие двух полученных значений метрики?\n",
    "\n",
    "***Задание 2 (0.5 балла).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Различие в метриках обусловлено выбором параметров в методе градиентного спуска (они отличаются от тех, которые заданы в изначальной функции LinearRegression).\n",
    "#Alpha слишком сильно наказывает за веса (возможно, они слишком большие)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1739612732157225e-25\n"
     ]
    }
   ],
   "source": [
    "reg = SGDRegressor(alpha=0).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вот и получилось через уменьшение параметра alpha сделать MSE с точностью до 25 знака таким же, как MSE у линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс. \n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n",
    "***Задание 4 (2 балла)***. Добавьте l1 (первый и второй варианты) или l2 (третий и четвертый варианты) регуляризацию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, alpha=0.5, l_ratio=0.000001, tol=0.001, max_iter=1000):\n",
    "        '''\n",
    "        Для начала необходимо инициализировать параметры\n",
    "        alpha - это learning rate или шаг обучения\n",
    "        l_ratio - параметр регуляризации\n",
    "        tol - значение для критерия останова\n",
    "        max_iter - максимальное количество итераций обучения\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "             \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Метод для обучения линейной регрессии\n",
    "        X - матрица признаков\n",
    "        y - вектор правильных ответов\n",
    "        '''\n",
    "        iteracii = 0\n",
    "        Xn = np.hstack([X, np.ones([X.shape[0], 1])])\n",
    "        w_0 = np.random.uniform(-2, 2, (Xn.shape[1])) #инициализирую вектор весов (рандомный)\n",
    "         \n",
    "        #так как я не знаю, идёт ли в исходной матрице первый столбец единичным, сам его добавляю\n",
    "        Xt = Xn.transpose()\n",
    "        \n",
    "        mse_0 = (1 / Xn.shape[0]) * np.linalg.norm((Xn @ w_0 - y))**2 + self.l_ratio * (np.linalg.norm(w_0[:-1])**2)\n",
    "        #считаю MSE для этих весов\n",
    "        crit = 100000\n",
    "        \n",
    "        while iteracii <= self.max_iter and crit >= self.tol: #условие на макс. кол-во итераций и критерий останова\n",
    "            grad_w = (2 / Xn.shape[0]) * Xt @ ((Xn @ w_0 - y)) + 2 * self.l_ratio * w_0\n",
    "             \n",
    "            w_1 = w_0 - self.alpha * grad_w\n",
    "            \n",
    "            mse_1 = (1 / Xn.shape[0]) * np.linalg.norm((Xn @ w_1 - y))**2 + self.l_ratio * (np.linalg.norm(w_1[:-1])**2)\n",
    "            crit = abs(mse_1 - mse_0)\n",
    "            w_0 = w_1\n",
    "            mse_0 = mse_1\n",
    "            iteracii += 1\n",
    "            \n",
    "        self.w_0 = w_0\n",
    "\n",
    "   \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Метод для предсказаний линейной регрессии\n",
    "        X - матрица признаков\n",
    "        '''\n",
    "        Xn = np.hstack([X, np.ones([X.shape[0], 1])])\n",
    "        return Xn.dot(self.w_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6451703761882005e-06\n",
      "You are amazing! Great work!\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression()\n",
    "my_reg.fit(X, y)\n",
    "print(mean_squared_error(y, my_reg.predict(X)))\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки\n",
    "\n",
    "* с l1-регуляризацией (from sklearn.linear_model import Lasso, **первый и второй вариант**) или с l2-регуляризацией (from sklearn.linear_model import Ridge, **третий и четвертый вариант**)\n",
    "* со значением параметра регуляризации **0.1 - для первого и третьего варианта, 0.01 - для второго и четвертого варианта**. \n",
    "\n",
    "Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9446973557210053e-08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "Ridge_1 = Ridge(alpha=0.01).fit(X, y)\n",
    "print(mean_squared_error(y, Ridge_1.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.870030682487219\n"
     ]
    }
   ],
   "source": [
    "Ne_Ridge = LinearRegression(l_ratio=0.01)\n",
    "Ne_Ridge.fit(X, y)\n",
    "print(mean_squared_error(y, Ne_Ridge.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вывод: получается, что мой код не так хорошо подбирает параметры, MSE больше в 100000000 раз. Метод из коробки работает лучше. Я программист начального уровня"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 6* (1 балл).***\n",
    "Пусть $P, Q \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_Q tr(PQ)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 7* (1 балл).***\n",
    "Пусть $x, y \\in \\mathbb{R}^{n}, M \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_M x^T M y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решения заданий 6 и 7 можно написать на листочке и отправить в anytask вместе с заполненным ноутбуком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
